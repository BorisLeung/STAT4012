{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4326d949-6dfd-4cbc-af5e-82a4a4062623",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from itertools import combinations\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8300eb2-89ce-4a2a-8b29-ffb86eb3fa51",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4b557cb7-cf64-42e5-af1b-0f5cd42513db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_poster2(movie_id):\n",
    "    poster_path = os.path.join(f\"posters\\\\{movie_id}.jpg\")\n",
    "    img = mpimg.imread(poster_path)\n",
    "    return img\n",
    "def indicator_function_1(data):\n",
    "    if (data >= 1970) and (data < 1980):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0 \n",
    "def indicator_function_2(data):\n",
    "    if (data >= 1980) and (data < 1990):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0 \n",
    "def indicator_function_3(data):\n",
    "    if (data >= 1990) and (data < 2000):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "     \n",
    "def indicator_function_4(data):\n",
    "    if (data >= 2000) and (data < 2010):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0    \n",
    "def indicator_function_5(data):\n",
    "    if (data >= 2010) and (data < 2020):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0    \n",
    "def indicator_function_6(data):\n",
    "    if (data >= 2020) and (data < 2030):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "def evaluate_auroc(y_pred, y_true, genres_names):\n",
    "    roc_auc_scores = {}\n",
    "    for i in range(y_true.shape[1]):  # Iterate over each label\n",
    "        roc_auc_scores[genres_names[i]] = roc_auc_score(y_true[:, i], y_pred[:, i])\n",
    "    \n",
    "    results = pd.DataFrame(roc_auc_scores.items(), columns=[\"Genre\", \"ROC AUC\"])\n",
    "    print(results)\n",
    "    average_roc_auc = np.mean(list(roc_auc_scores.values()))\n",
    "    print(f\"Average ROC AUC: {average_roc_auc:.6f}\")\n",
    "    \n",
    "    for i in range(y_true.shape[1]):\n",
    "        fpr, tpr, _ = roc_curve(y_true[:, i], y_pred[:, i])\n",
    "        plt.plot(fpr, tpr, label=genres_names[i])\n",
    "    plt.plot([0, 1], [0, 1], color=\"red\", linestyle=\"--\", label=\"Baseline\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(f\"{genres_names[0]} {genres_names[1]} {genres_names[2]} ROC curve\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fede796-d434-479e-9b12-6f8584e6b4c4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3e5f103e-3279-40ef-b77e-4c867fe27bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Activation, BatchNormalization, GroupNormalization, Dropout, Conv2D, MaxPooling2D, Flatten, Concatenate, Input\n",
    "n_genre = 3\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), activation ='relu', padding = 'Same', input_shape =(345, 230, 3))) #1\n",
    "model.add(GroupNormalization())\n",
    "model.add(Conv2D(32, (3, 3), activation ='relu', padding = 'Same')) #2\n",
    "model.add(GroupNormalization())\n",
    "model.add(Conv2D(32, (3, 3), activation ='relu', padding = 'Same')) #3\n",
    "model.add(GroupNormalization())\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "    \n",
    "model.add(Conv2D(32, (3, 3), activation ='relu', padding = 'Same')) #4\n",
    "model.add(GroupNormalization())\n",
    "model.add(Conv2D(32, (3, 3), activation ='relu', padding = 'Same')) #5\n",
    "model.add(GroupNormalization())\n",
    "model.add(Conv2D(32, (3, 3), activation ='relu', padding = 'Same')) #6\n",
    "model.add(GroupNormalization())\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "    \n",
    "model.add(Conv2D(32, (3, 3), activation ='relu', padding = 'Same')) #7\n",
    "model.add(GroupNormalization())\n",
    "model.add(Conv2D(32, (3, 3), activation ='relu', padding = 'Same')) #8\n",
    "model.add(GroupNormalization())\n",
    "model.add(Conv2D(32, (3, 3), activation ='relu', padding = 'Same')) #9\n",
    "model.add(GroupNormalization())\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "    \n",
    "model.add(Conv2D(64, (3, 3), activation ='relu', padding = 'Same')) #10\n",
    "model.add(GroupNormalization())\n",
    "model.add(Conv2D(64, (3, 3), activation ='relu', padding = 'Same')) #11\n",
    "model.add(GroupNormalization())\n",
    "model.add(Conv2D(64, (3, 3), activation ='relu', padding = 'Same')) #12\n",
    "model.add(GroupNormalization())\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "    \n",
    "model.add(Flatten())\n",
    "indicator_inputs = Input(shape=(6,))\n",
    "concatenated_inputs = Concatenate()([model.output, indicator_inputs])\n",
    "    \n",
    "x = BatchNormalization()(concatenated_inputs)\n",
    "x = Dense(32, activation='relu')(x)\n",
    "x = Dropout(0.25)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "output = Dense(n_genre, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=[model.input, indicator_inputs], outputs=output)\n",
    "model.compile(optimizer='adam', loss=tensorflow.keras.losses.CategoricalCrossentropy(), metrics=['accuracy'])\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248755fc-14e7-44cf-b4fa-26d91c827da4",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ec299dc-6385-4fa2-9a6b-1baebc967d2f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['_Comedy' '_Drama' '_Romance']\n",
      "Image fetched---------------------------------\n",
      "Train-test Splited---------------------------------\n",
      "Epoch 1/5\n",
      "WARNING:tensorflow:From C:\\Users\\user\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\user\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "180/180 [==============================] - 814s 4s/step - loss: 1.2376 - accuracy: 0.3484 - val_loss: 1.0824 - val_accuracy: 0.4007\n",
      "Epoch 2/5\n",
      "180/180 [==============================] - 795s 4s/step - loss: 1.1193 - accuracy: 0.3880 - val_loss: 1.0808 - val_accuracy: 0.3875\n",
      "Epoch 3/5\n",
      "180/180 [==============================] - 961s 5s/step - loss: 1.0833 - accuracy: 0.4057 - val_loss: 1.0544 - val_accuracy: 0.4326\n",
      "Epoch 4/5\n",
      "180/180 [==============================] - 1111s 6s/step - loss: 1.0704 - accuracy: 0.4156 - val_loss: 1.0455 - val_accuracy: 0.4437\n",
      "Epoch 5/5\n",
      "180/180 [==============================] - 1110s 6s/step - loss: 1.0449 - accuracy: 0.4437 - val_loss: 1.0599 - val_accuracy: 0.4125\n",
      "57/57 [==============================] - 158s 3s/step\n",
      "Accuracy: 0.41944444444444445\n",
      "Confusion Matrix:\n",
      "[[125 288 188]\n",
      " [122 334 142]\n",
      " [117 188 296]]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'evaluate_auroc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 104\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConfusion Matrix:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28mprint\u001b[39m(confusion)\n\u001b[1;32m--> 104\u001b[0m \u001b[43mevaluate_auroc\u001b[49m(y_pred, y_test, genre_rank_filter)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'evaluate_auroc' is not defined"
     ]
    }
   ],
   "source": [
    "np.random.seed(4749)\n",
    "genre_choices = ['_Action', '_Animation', '_Comedy', '_Documentary', '_Drama', '_Horror', '_Music', '_Romance']\n",
    "genre_choices_list = list(combinations(genre_choices, 3))\n",
    "# 17: [\"Action\", \"Drama\", \"Romance\"]\n",
    "# 42: [\"Comedy\", \"Drama\", \"Romance\"]\n",
    "# 52: [\"Drama\", \"Horror\", \"Music\"] \n",
    "best_models_index = [17, 42, 52]\n",
    "for i in best_models_index:\n",
    "    genre_rank_filter = np.array(genre_choices_list[i])\n",
    "    print(genre_rank_filter)\n",
    "    \n",
    "    n_genre = 3\n",
    "    IMG_SIZE = (345, 230, 3)\n",
    "    size = 3000\n",
    "    test_portion = 0.2\n",
    "    \n",
    "    genre_df = pd.read_csv(\"encoded_genres.csv\", index_col=0)\n",
    "    genre_df[\"date\"] = pd.to_datetime(genre_df[\"date\"]) \n",
    "    genre_df = genre_df[genre_df[\"date\"].dt.year >= 2000]\n",
    "    genre_df_filtered = genre_df[genre_rank_filter]\n",
    "    genre_filtered = genre_df_filtered[np.sum(genre_df_filtered * 1, axis=1) == 1]\n",
    "    movie_img = []\n",
    "    movie_genre = []\n",
    "    year_list = []\n",
    "    \n",
    "    for i in range(n_genre):\n",
    "        genre_i = genre_filtered.loc[genre_filtered.iloc[:, i] == True]\n",
    "        rand = np.random.randint(0, genre_i.shape[0], size=(size + 10))\n",
    "        n_image = 0\n",
    "        j = 0\n",
    "        while n_image < size:\n",
    "            index = genre_filtered.index[rand[j]]\n",
    "            year = datetime.strptime(genre_df[\"date\"].iloc[np.where(genre_df.index == index)[0].item()],\n",
    "                                     \"%Y-%m-%d\").year\n",
    "            temp = get_poster2(index)\n",
    "            if sum(temp.shape) == sum(IMG_SIZE):\n",
    "                movie_img.append(temp)\n",
    "                movie_genre.append(genre_i.iloc[rand[j], :n_genre])\n",
    "                n_image += 1\n",
    "                year_list.append(year)\n",
    "            j = j + 1\n",
    "    \n",
    "    year_list = np.array(year_list)\n",
    "    year_list = np.reshape(year_list, (size*n_genre, 1))\n",
    "    movie_img = np.array(movie_img, dtype=float)\n",
    "    movie_genre = np.array(movie_genre, dtype=float).reshape((size * n_genre, n_genre))\n",
    "    genre_year = np.hstack((movie_genre, year_list))\n",
    "    print(\"Image fetched---------------------------------\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(movie_img, genre_year, test_size=test_portion, random_state=4748)\n",
    "    X_train = X_train / 255.0\n",
    "    X_test = X_test / 255.0\n",
    "    year_list_train = y_train[:, n_genre:].astype(int)\n",
    "    y_train = y_train[:, :n_genre]\n",
    "    year_list_test = y_test[:, n_genre:].astype(int)\n",
    "    y_test = y_test[:, :n_genre]\n",
    "    print(\"Train-test Splited---------------------------------\")\n",
    "    train_size = int(size * n_genre * (1 - test_portion))\n",
    "    indicator_1 = [indicator_function_1(x) for x in year_list_train]\n",
    "    indicator_1 = np.reshape(indicator_1, (train_size, 1))\n",
    "    indicator_2 = [indicator_function_2(x) for x in year_list_train]\n",
    "    indicator_2 = np.reshape(indicator_2, (train_size, 1))\n",
    "    indicator_3 = [indicator_function_3(x) for x in year_list_train]\n",
    "    indicator_3 = np.reshape(indicator_3, (train_size, 1))\n",
    "    indicator_4 = [indicator_function_4(x) for x in year_list_train]\n",
    "    indicator_4 = np.reshape(indicator_4, (train_size,1))\n",
    "    indicator_5 = [indicator_function_5(x) for x in year_list_train]\n",
    "    indicator_5 = np.reshape(indicator_5, (train_size,1))\n",
    "    indicator_6 = [indicator_function_6(x) for x in year_list_train]\n",
    "    indicator_6 = np.reshape(indicator_6, (train_size,1))\n",
    "    \n",
    "    train_indicators = np.concatenate((indicator_1, indicator_2, indicator_3, indicator_4, indicator_5, indicator_6), axis=1)\n",
    "    training_model = model.fit([X_train, train_indicators], y_train, batch_size=32, epochs=5, validation_split=0.2)\n",
    "    \n",
    "    test_size = int(size*n_genre*test_portion)\n",
    "    indicator_1 = [indicator_function_1(x) for x in year_list_test]\n",
    "    indicator_1 = np.reshape(indicator_1, (test_size,1))\n",
    "    indicator_2 = [indicator_function_2(x) for x in year_list_test]\n",
    "    indicator_2 = np.reshape(indicator_2, (test_size,1))\n",
    "    indicator_3 = [indicator_function_3(x) for x in year_list_test]\n",
    "    indicator_3 = np.reshape(indicator_3, (test_size,1))\n",
    "    indicator_4 = [indicator_function_4(x) for x in year_list_test]\n",
    "    indicator_4 = np.reshape(indicator_4, (test_size,1))\n",
    "    indicator_5 = [indicator_function_5(x) for x in year_list_test]\n",
    "    indicator_5 = np.reshape(indicator_5, (test_size,1))\n",
    "    indicator_6 = [indicator_function_6(x) for x in year_list_test]\n",
    "    indicator_6 = np.reshape(indicator_6, (test_size,1))\n",
    "    \n",
    "    test_indicators = np.concatenate((indicator_1, indicator_2, indicator_3, indicator_4, indicator_5, indicator_6), axis=1)\n",
    "    y_pred = model.predict([X_test,test_indicators])\n",
    "    pred_y = np.argmax(y_pred, axis=1)\n",
    "    true_y = np.argmax(y_test, axis=1)\n",
    "    accuracy = np.mean(true_y == pred_y)\n",
    "    confusion = confusion_matrix(true_y, pred_y)\n",
    "    \n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion)\n",
    "    \n",
    "    evaluate_auroc(y_pred, y_test, genre_rank_filter)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
